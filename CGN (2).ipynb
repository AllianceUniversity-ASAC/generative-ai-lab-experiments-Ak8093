{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The Case Context (E-commerce example)\n",
        "\n",
        "The company wants to create synthetic product mockups (like shirts, shoes, bags) before manufacturing.\n",
        "\n",
        "They need to control which product appears in the generated image — i.e., not random but chosen (“generate shoes only”).\n",
        "\n",
        " What You Have to Do\n",
        "\n",
        "Implement a CGAN using Fashion-MNIST\n",
        "(Fashion-MNIST is a free dataset of 28×28 grayscale images of clothes, shoes, bags, etc.)\n",
        "\n",
        "Train the CGAN so that if you give it noise + label (“shoe”), it will generate a shoe-like image.\n",
        "\n",
        "Compare your CGAN with a normal GAN (no labels) to show how much better conditional generation is.\n",
        "\n",
        "Analyze training stability: GANs are tricky to train (they can collapse or produce noise). You have to explain:\n",
        "\n",
        "Did you use label smoothing?\n",
        "\n",
        "Did you use batch normalization?\n",
        "\n",
        "How did you balance generator vs. discriminator training?\n",
        "\n",
        " Expected Deliverables (What you submit)\n",
        "\n",
        "Working CGAN code (training logs printed so teachers can see the loss decreasing).\n",
        "\n",
        "Visual results: e.g. a grid of generated images for each label (“all shoes in one row, all bags in another”).\n",
        "\n",
        "Discussion/comparison:\n",
        "\n",
        "Show how a plain GAN produces random clothes but a CGAN can be told “only shoes”.\n",
        "\n",
        "Talk about convergence: which model was more stable? What tricks you used to avoid mode collapse.\n",
        "\n",
        "Maybe a short report summarizing your findings.\n",
        "\n",
        " Key Idea in One Sentence\n",
        "\n",
        "A CGAN is like a GAN with an extra steering wheel — you don’t just generate random fashion images, you steer it to produce a specific category like “shoes” or “bags.”"
      ],
      "metadata": {
        "id": "-TO4seJQO03Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gut_uvt3MhBP",
        "outputId": "97cbf765-dd31-4f8b-872d-072f75db962c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 469/469 [00:17<00:00, 26.78it/s, lossD_real=0.601, lossD_fake=0.641, lossG=1.41]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved sample: cgan_samples/epoch_001.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 469/469 [00:15<00:00, 30.29it/s, lossD_real=0.928, lossD_fake=0.369, lossG=1.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved sample: cgan_samples/epoch_002.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 469/469 [00:15<00:00, 30.61it/s, lossD_real=0.613, lossD_fake=0.677, lossG=1.45]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved sample: cgan_samples/epoch_003.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 469/469 [00:15<00:00, 29.73it/s, lossD_real=0.822, lossD_fake=0.484, lossG=1.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved sample: cgan_samples/epoch_004.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 469/469 [00:15<00:00, 30.98it/s, lossD_real=0.556, lossD_fake=0.355, lossG=1.43]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved sample: cgan_samples/epoch_005.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 469/469 [00:15<00:00, 30.34it/s, lossD_real=0.604, lossD_fake=0.335, lossG=1.58]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved sample: cgan_samples/epoch_006.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 469/469 [00:15<00:00, 30.61it/s, lossD_real=0.743, lossD_fake=0.289, lossG=1.26]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved sample: cgan_samples/epoch_007.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 469/469 [00:16<00:00, 29.26it/s, lossD_real=0.63, lossD_fake=0.326, lossG=1.73]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved sample: cgan_samples/epoch_008.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 469/469 [00:16<00:00, 28.79it/s, lossD_real=0.476, lossD_fake=0.773, lossG=1.81]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved sample: cgan_samples/epoch_009.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 469/469 [00:15<00:00, 30.42it/s, lossD_real=0.739, lossD_fake=0.268, lossG=1.7]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved sample: cgan_samples/epoch_010.png\n",
            "Training finished. Samples in cgan_samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# CGAN on Fashion-MNIST (PyTorch) - Colab-ready\n",
        "# Run this in Google Colab (GPU recommended)\n",
        "\n",
        "# 1) Install / imports\n",
        "!pip install -q torch torchvision tqdm matplotlib\n",
        "import os, math, random, time\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid, save_image\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# 2) Hyperparams\n",
        "img_size = 28\n",
        "nc = 1\n",
        "nz = 100        # noise dim\n",
        "n_classes = 10  # Fashion-MNIST labels\n",
        "embed_dim = 50\n",
        "ngf = 64\n",
        "ndf = 64\n",
        "batch_size = 128\n",
        "lr = 2e-4\n",
        "beta1 = 0.5\n",
        "n_epochs = 10\n",
        "sample_dir = \"cgan_samples\"\n",
        "os.makedirs(sample_dir, exist_ok=True)\n",
        "\n",
        "# 3) Dataset + loader\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "trainset = torchvision.datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "# 4) Models: conditional via label embedding -> concat with noise / image\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, nz, embed_dim, n_classes, ngf):\n",
        "        super().__init__()\n",
        "        self.label_emb = nn.Embedding(n_classes, embed_dim)\n",
        "        self.net = nn.Sequential(\n",
        "            # input: nz + embed_dim\n",
        "            nn.Linear(nz + embed_dim, ngf*4*7*7),\n",
        "            nn.BatchNorm1d(ngf*4*7*7),\n",
        "            nn.ReLU(True),\n",
        "            # reshape to (ngf*4, 7, 7)\n",
        "            View((-1, ngf*4, 7, 7)),\n",
        "            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False), # 14x14\n",
        "            nn.BatchNorm2d(ngf*2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),   # 28x28\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(ngf, nc, 3, 1, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    def forward(self, z, labels):\n",
        "        le = self.label_emb(labels)\n",
        "        x = torch.cat([z, le], dim=1)\n",
        "        return self.net(x)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, n_classes, embed_dim, ndf):\n",
        "        super().__init__()\n",
        "        self.label_emb = nn.Embedding(n_classes, embed_dim)\n",
        "        # we'll expand label embedding to an image-sized channel and concat with input\n",
        "        self.project = nn.Sequential(\n",
        "            nn.Linear(embed_dim, img_size*img_size),\n",
        "        )\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(nc+1, ndf, 4, 2, 1), # 14x14\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf, ndf*2, 4, 2, 1), # 7x7\n",
        "            nn.BatchNorm2d(ndf*2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf*2, 1, 7, 1, 0), # 1x1\n",
        "        )\n",
        "    def forward(self, img, labels):\n",
        "        le = self.label_emb(labels)\n",
        "        proj = self.project(le).view(-1, 1, img_size, img_size)\n",
        "        x = torch.cat([img, proj], dim=1)\n",
        "        out = self.net(x)\n",
        "        return out.view(-1)\n",
        "\n",
        "# helper view layer\n",
        "class View(nn.Module):\n",
        "    def __init__(self, shape):\n",
        "        super().__init__()\n",
        "        self.shape = shape\n",
        "    def forward(self, x):\n",
        "        return x.view(*self.shape)\n",
        "\n",
        "# instantiate\n",
        "netG = Generator(nz, embed_dim, n_classes, ngf).to(device)\n",
        "netD = Discriminator(n_classes, embed_dim, ndf).to(device)\n",
        "\n",
        "# init weights\n",
        "def weights_init(m):\n",
        "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "netG.apply(weights_init); netD.apply(weights_init)\n",
        "\n",
        "# 5) Losses + optim\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "# labels for BCE\n",
        "real_label = 1.0\n",
        "fake_label = 0.0\n",
        "\n",
        "# fixed noise for visualization (10 classes x 8 samples each)\n",
        "fixed_noise = torch.randn(n_classes*8, nz, device=device)\n",
        "fixed_labels = torch.tensor([i for i in range(n_classes) for _ in range(8)], dtype=torch.long, device=device)\n",
        "\n",
        "# 6) Training loop\n",
        "iter_count = 0\n",
        "for epoch in range(n_epochs):\n",
        "    pbar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{n_epochs}\")\n",
        "    for i, (imgs, labels) in enumerate(pbar):\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        bsize = imgs.size(0)\n",
        "        # Train D\n",
        "        netD.zero_grad()\n",
        "        # real\n",
        "        out_real = netD(imgs, labels)\n",
        "        # label smoothing (helps stability): real target slightly less than 1.0\n",
        "        real_targets = torch.full((bsize,), 0.9, device=device)\n",
        "        lossD_real = criterion(out_real, real_targets)\n",
        "        lossD_real.backward()\n",
        "        # fake\n",
        "        noise = torch.randn(bsize, nz, device=device)\n",
        "        rand_labels = torch.randint(0, n_classes, (bsize,), device=device)\n",
        "        fake = netG(noise, rand_labels)\n",
        "        out_fake = netD(fake.detach(), rand_labels)\n",
        "        fake_targets = torch.zeros(bsize, device=device)\n",
        "        lossD_fake = criterion(out_fake, fake_targets)\n",
        "        lossD_fake.backward()\n",
        "        optimizerD.step()\n",
        "        # Train G\n",
        "        netG.zero_grad()\n",
        "        out_fake2 = netD(fake, rand_labels)\n",
        "        # generator wants discriminator to predict real (1)\n",
        "        lossG = criterion(out_fake2, torch.ones(bsize, device=device))\n",
        "        lossG.backward()\n",
        "        optimizerG.step()\n",
        "\n",
        "        iter_count += 1\n",
        "        if i % 200 == 0:\n",
        "            pbar.set_postfix({'lossD_real': lossD_real.item(), 'lossD_fake': lossD_fake.item(), 'lossG': lossG.item()})\n",
        "\n",
        "    # Save sample grid after each epoch\n",
        "    netG.eval()\n",
        "    with torch.no_grad():\n",
        "        samples = netG(fixed_noise, fixed_labels).cpu()\n",
        "    netG.train()\n",
        "    # unnormalize from [-1,1] to [0,1]\n",
        "    grid = make_grid((samples+1)/2, nrow=8, padding=2)\n",
        "    save_image(grid, os.path.join(sample_dir, f\"epoch_{epoch+1:03d}.png\"))\n",
        "    print(\"Saved sample:\", os.path.join(sample_dir, f\"epoch_{epoch+1:03d}.png\"))\n",
        "\n",
        "print(\"Training finished. Samples in\", sample_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YocqfmlOOzfs"
      }
    }
  ]
}